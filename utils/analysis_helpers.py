# --- START OF FILE utils/analysis_helpers.py ---
import re
from collections import Counter
from typing import List, Tuple, Dict, Any # <-- Добавили Dict, Any

# Можно добавить стоп-слова из файла или библиотеки (nltk, spacy)
# Загрузка стоп-слов (пример)
# try:
#     from nltk.corpus import stopwords
#     STOP_WORDS = set(stopwords.words('russian'))
#     # Добавляем специфичные для бота слова
#     STOP_WORDS.update(['бот', 'пожалуйста', 'спасибо', 'здравствуйте', 'привет',
#                       'вопрос', 'ответ', 'текст', 'ссылка', 'документ', 'изображение',
#                       'изложить', 'перевести', 'план', 'доход', 'про', 'нас', 'какой',
#                       'который', 'хотеть', 'мочь', 'делать', 'сказать', 'использовать'])
# except ImportError:
#     print("NLTK не найден. Используются базовые стоп-слова.")
#     # Базовый набор стоп-слов, если NLTK не установлен
# Расширяем базовый набор стоп-слов
STOP_WORDS = set([
    'и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то',
    'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за',
    'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет',
    'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если',
    'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять',
    'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может',
    'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем',
    'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под',
    'жи', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой',
    'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы',
    'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при',
    'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот',
    'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве',
    'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда',
    'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно',
    'изображение', 'картинка', 'опиши', 'подробно', 'изображено', 'справку', 'энциклопедическую', 'предоставь',
    'всю', 'между',
    # Специфичные для бота/общения слова
    'бот', 'gemini', 'чат', 'пожалуйста', 'спасибо', 'привет', 'здравствуй', 'пока',
    'вопрос', 'ответ', 'текст', 'сообщение', 'файл', 'картинка', 'изображение', 'ссылка',
    'перевести', 'перевод', 'язык', 'изложить', 'кратко', 'суть', 'тема',
    'план', 'доход', 'деньги', 'заработать', 'цель',
    'напоминание', 'напомнить', 'время', 'дата', 'установить',
    'история', 'диалог', 'посмотреть',
    'настройки', 'стиль', 'таймзона', 'пояс',
    'помощь', 'справка', 'умеешь', 'можешь', 'функции',
    'делать', 'сделать', 'хотеть', 'мочь', 'использовать', 'сказать', 'рассказать', 'дать',
    'который', 'какой', 'это', 'также'
])


def extract_frequent_topics(conversation_history: List[Dict[str, Any]], top_n: int = 7, min_word_length: int = 4) -> List[str]:
    """
    Извлекает наиболее частые ключевые слова (темы) из истории разговоров пользователя.
    Теперь принимает List[Dict[str, Any]] вместо List[Tuple[str, str]].

    Args:
        conversation_history: Список словарей {'role': str, 'message_text': str}.
        top_n: Количество тем для возврата.
        min_word_length: Минимальная длина слова для учета (увеличено до 4).

    Returns:
        Список строк (тем).
    """
    if not conversation_history:
        print("DEBUG: История разговоров пуста.")
        return []

    # Собираем только сообщения пользователя ('user')
    user_messages_text = " ".join([
        item.get('message_text', '').lower()
        for item in conversation_history
        if item.get('role') == 'user' and item.get('message_text')
    ])
    print(f"DEBUG: Текст сообщений пользователя: {user_messages_text}")

    if not user_messages_text:
        print("DEBUG: Текст сообщений пользователя пуст.")
        return []

    # Очистка текста: оставляем только кириллицу, латиницу и пробелы
    # Удаляем цифры и прочие символы
    cleaned_text = re.sub(r'[^а-яёa-z\s]', '', user_messages_text)
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip() # Убираем лишние пробелы
    print(f"DEBUG: Очищенный текст: {cleaned_text}")

    # Разбиваем на слова
    words = cleaned_text.split()
    print(f"DEBUG: Слова после разбиения: {words}")

    # Фильтруем слова: убираем стоп-слова и короткие слова
    filtered_words = [
        word for word in words
        if word not in STOP_WORDS and len(word) >= min_word_length
    ]
    print(f"DEBUG: Отфильтрованные слова: {filtered_words}")

    if not filtered_words:
        print("DEBUG: Отфильтрованные слова пусты.")
        return []

    # Подсчитываем частоту слов
    word_counts = Counter(filtered_words)
    print(f"DEBUG: Частота слов: {word_counts}")

    # Получаем top_n наиболее частых слов
    top_topics = [word for word, count in word_counts.most_common(top_n)]
    print(f"DEBUG: Top {top_n} тем: {top_topics}")

    return top_topics

# --- END OF FILE utils/analysis_helpers.py ---